{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-09T13:57:43.270959Z","iopub.status.busy":"2024-09-09T13:57:43.270571Z","iopub.status.idle":"2024-09-09T13:57:43.865445Z","shell.execute_reply":"2024-09-09T13:57:43.864525Z","shell.execute_reply.started":"2024-09-09T13:57:43.270921Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/video-id-2-img/video_id2img_id.json\n","/kaggle/input/clip-feature/clip-features-32/L06_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V031.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V031.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V011.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V012.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V031.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V027.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V022.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V001.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V031.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V026.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V006.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V031.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V021.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V015.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V014.npy\n","/kaggle/input/clip-feature/clip-features-32/L11_V020.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V016.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V008.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V023.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V013.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V005.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V019.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V030.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V010.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L07_V028.npy\n","/kaggle/input/clip-feature/clip-features-32/L05_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V003.npy\n","/kaggle/input/clip-feature/clip-features-32/L01_V018.npy\n","/kaggle/input/clip-feature/clip-features-32/L06_V002.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V017.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V009.npy\n","/kaggle/input/clip-feature/clip-features-32/L08_V007.npy\n","/kaggle/input/clip-feature/clip-features-32/L10_V025.npy\n","/kaggle/input/clip-feature/clip-features-32/L03_V004.npy\n","/kaggle/input/clip-feature/clip-features-32/L12_V029.npy\n","/kaggle/input/clip-feature/clip-features-32/L09_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L02_V024.npy\n","/kaggle/input/clip-feature/clip-features-32/L04_V011.npy\n","/kaggle/input/output/output.json\n","/kaggle/input/id2img-fps/id2img_fps.json\n","/kaggle/input/bin-file/faiss_clipv2_cosine.bin\n","/kaggle/input/video-id2imgid/video_id2img_id.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T13:58:23.148519Z","iopub.status.busy":"2024-09-09T13:58:23.148178Z","iopub.status.idle":"2024-09-09T13:58:49.253392Z","shell.execute_reply":"2024-09-09T13:58:49.252252Z","shell.execute_reply.started":"2024-09-09T13:58:23.148485Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: faiss-gpu in /opt/conda/lib/python3.10/site-packages (1.7.2)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: openai-clip in /opt/conda/lib/python3.10/site-packages (1.0.1)\n","Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from openai-clip) (6.2.3)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from openai-clip) (2024.5.15)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from openai-clip) (4.66.4)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->openai-clip) (0.2.13)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install faiss-gpu\n","%pip install openai-clip"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T13:59:06.629013Z","iopub.status.busy":"2024-09-09T13:59:06.628522Z","iopub.status.idle":"2024-09-09T13:59:22.059040Z","shell.execute_reply":"2024-09-09T13:59:22.057818Z","shell.execute_reply.started":"2024-09-09T13:59:06.628959Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-0_57i8cx\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-0_57i8cx\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.5.15)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.4)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.4.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.19.0)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.1.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.6.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install git+https://github.com/openai/CLIP.git\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T13:59:50.534623Z","iopub.status.busy":"2024-09-09T13:59:50.534191Z","iopub.status.idle":"2024-09-09T14:00:03.822995Z","shell.execute_reply":"2024-09-09T14:00:03.821808Z","shell.execute_reply.started":"2024-09-09T13:59:50.534577Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: open_clip_torch in /opt/conda/lib/python3.10/site-packages (2.26.1)\n","Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2.4.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.19.0)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2024.5.15)\n","Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (6.2.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (4.66.4)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.24.6)\n","Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (1.0.8)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open_clip_torch) (2024.6.1)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.13)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (6.0.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open_clip_torch) (0.4.4)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open_clip_torch) (9.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open_clip_torch) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install open_clip_torch\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T14:42:34.988426Z","iopub.status.busy":"2024-09-09T14:42:34.988031Z","iopub.status.idle":"2024-09-09T14:42:35.001030Z","shell.execute_reply":"2024-09-09T14:42:34.999951Z","shell.execute_reply.started":"2024-09-09T14:42:34.988382Z"},"trusted":true},"outputs":[],"source":["import faiss\n","import open_clip\n","import numpy as np\n","import json\n","import torch\n","\n","class Text_Search:\n","    def __init__(self, clipv2_bin: str, keyframe_file_json: str):\n","        self.clip_index = self.load_bin_file(clipv2_bin)\n","        self.id2img_fps = self.load_json_file(keyframe_file_json)\n","        # Load model\n","        self._device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.clipv2_model, _, _ = open_clip.create_model_and_transforms('ViT-L-14', device=self._device, pretrained='datacomp_xl_s13b_b90k')\n","        self.clipv2_tokenizer = open_clip.get_tokenizer('ViT-L-14')\n","\n","    def load_bin_file(self, bin_file):\n","        return faiss.read_index(bin_file)\n","\n","    def load_json_file(self, json_file):\n","        with open(json_file, 'r') as f:\n","            js = json.load(f)\n","        return js\n","\n","    def find_video_info(self, index_images):\n","        for keys, values in self.id2img_fps.items():\n","            if int(keys)== index_images:\n","                return values\n","        return None\n","\n","    def text_search(self, queries: str, k: int, index=None):\n","        \"\"\"Implement text search in database\"\"\"\n","        text = self.clipv2_tokenizer([queries]).to(self._device)\n","        text_features = self.clipv2_model.encode_text(text)\n","        text_features /= text_features.norm(dim=-1, keepdim=True)\n","        text_features = text_features.cpu().detach().numpy().astype(np.float32)\n","\n","        if index is None:\n","            scores, index_image = self.clip_index.search(text_features, k=k)\n","        else:\n","            id_selector = faiss.IDSelectorArray(index)\n","            scores, index_image = self.clip_index.search(text_features, k=k, params=faiss.SearchParametersIVF(id_selector))\n","\n","        idx_image = index_image.flatten()\n","        infos_query = [self.find_video_info(idx) for idx in idx_image]  # Ensure idx is a valid index\n","        link_paths= [info[\"image_path\"] for info in infos_query]\n","        return scores.flatten(), idx_image,link_paths\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T14:40:54.494917Z","iopub.status.busy":"2024-09-09T14:40:54.494592Z","iopub.status.idle":"2024-09-09T14:40:55.379069Z","shell.execute_reply":"2024-09-09T14:40:55.378054Z","shell.execute_reply.started":"2024-09-09T14:40:54.494882Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'image_path': '/mnt/f/Luan/AIHCM/KeyFrames/L04_extra/V012/011015.jpg', 'scene_idx': 'L04/V012/lst_shot/136'}\n"]}],"source":["import json\n","\n","def load_json_file(json_file: str):\n","    with open(json_file, \"r\") as f:\n","        js = json.load(f)\n","    return js  # Return the loaded JSON data, not the file object\n","\n","def find_video_info(json_file: str, index: int):\n","    video_data = load_json_file(json_file)\n","    for keys,value in video_data.items():\n","        if int(keys) ==index:\n","            return value \n","    return None\n","\n","# Example usage\n","json_file = r\"/kaggle/input/id2img-fps/id2img_fps.json\"\n","print(find_video_info(json_file, 128924))\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T15:10:22.017257Z","iopub.status.busy":"2024-09-09T15:10:22.016542Z","iopub.status.idle":"2024-09-09T15:10:40.013030Z","shell.execute_reply":"2024-09-09T15:10:40.011964Z","shell.execute_reply.started":"2024-09-09T15:10:22.017215Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting googletrans\n","  Downloading googletrans-3.0.0.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting httpx==0.13.3 (from googletrans)\n","  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2024.7.4)\n","Collecting hstspreload (from httpx==0.13.3->googletrans)\n","  Downloading hstspreload-2024.9.1-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.3.1)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n","Collecting idna==2.* (from httpx==0.13.3->googletrans)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n","  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n","Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n","Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n","Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n","Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Downloading hstspreload-2024.9.1-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=233277881cbff30d10c52af2f616f5e7fef3f7ffe07a53f529cc71657ae3cc38\n","  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: h11\n","    Found existing installation: h11 0.14.0\n","    Uninstalling h11-0.14.0:\n","      Successfully uninstalled h11-0.14.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.7\n","    Uninstalling idna-3.7:\n","      Successfully uninstalled idna-3.7\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.5\n","    Uninstalling httpcore-1.0.5:\n","      Successfully uninstalled httpcore-1.0.5\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.27.0\n","    Uninstalling httpx-0.27.0:\n","      Successfully uninstalled httpx-0.27.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastapi 0.111.0 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","jupyterlab 4.2.4 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\n","jupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","ydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install googletrans"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-09-09T14:42:47.779117Z","iopub.status.busy":"2024-09-09T14:42:47.778731Z","iopub.status.idle":"2024-09-09T14:42:56.303879Z","shell.execute_reply":"2024-09-09T14:42:56.302434Z","shell.execute_reply.started":"2024-09-09T14:42:47.779083Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Image Indexes: [ 77722 192833  77686  77685 392138  77721  77720  69583  77889  77684]\n","Image path  ['/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/011763.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L06_extra/V001/013548.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/010938.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/010884.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L10_extra/V026/017244.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/011634.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/011505.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L02_extra/V026/010733.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/018119.jpg', '/mnt/f/Luan/AIHCM/KeyFrames/L03_extra/V001/010830.jpg']\n"]}],"source":["link_bin_file = r\"/kaggle/input/bin-file/faiss_clipv2_cosine.bin\"\n","json_file = r\"/kaggle/input/id2img-fps/id2img_fps.json\"\n","\n","text_searcher = Text_Search(link_bin_file, json_file)\n","input_test = \" \"\n","scores, idx_image , image_path= text_searcher.text_search(input_test, k=10, index=None)  # Ensure all three values are returned\n","\n","print(\"Image Indexes:\", idx_image)\n","\n","print(\"Image path \", image_path)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5655808,"sourceId":9333849,"sourceType":"datasetVersion"},{"datasetId":5655820,"sourceId":9333868,"sourceType":"datasetVersion"},{"datasetId":5656021,"sourceId":9334125,"sourceType":"datasetVersion"},{"datasetId":5656254,"sourceId":9334481,"sourceType":"datasetVersion"},{"datasetId":5666539,"sourceId":9348692,"sourceType":"datasetVersion"},{"datasetId":5669529,"sourceId":9352744,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
